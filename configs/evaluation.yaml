# Evaluation Configuration
# ========================
# Settings for LLM-as-a-Judge evaluation pipeline
# Override via environment variables with EVALUATION__ prefix
# Example: EVALUATION__API_URL=http://localhost:8080

evaluation:
  # API endpoint to test
  # Default: http://localhost:8000
  api_url: http://localhost:8000

  # LLM configuration for evaluation judge
  # Uses GPT-4 with temperature=0 for consistent evaluation
  llm:
    provider: "litellm"
    proxy_url: http://localhost:4000
    api_key: "@jinja {{ env.get('LITELLM_API_KEY', '') }}"
    model: "gpt-4"
    temperature: 0.0
    max_tokens: 2000

  # Langfuse observability configuration
  # Required for workflow validation and score logging
  observability:
    langfuse:
      provider: "langfuse"
      host: "@jinja {{ env.get('LANGFUSE_HOST', 'https://cloud.langfuse.com') }}"
      public_key: "@jinja {{ env.get('LANGFUSE_PUBLIC_KEY', '') }}"
      secret_key: "@jinja {{ env.get('LANGFUSE_SECRET_KEY', '') }}"

  # Workflow validation settings
  workflow:
    # Seconds to wait for Langfuse trace submission after API call
    trace_wait_time: 30
    # Maximum retries when fetching trace from Langfuse
    max_retries: 10
    # Delay between retry attempts (seconds)
    retry_delay: 3

  # Score thresholds for pass/fail determination
  # Scenarios scoring below these thresholds are flagged
  thresholds:
    # LLM judge quality metrics (0-1 scale)
    answer_quality: 0.7
    factual_correctness: 0.8
    completeness: 0.7
    # Classification accuracy (0-1 scale, 1.0 = exact match)
    urgency_accuracy: 0.9
    routing_accuracy: 0.9
    action_accuracy: 0.9

  # Prompt configuration for LLM judge
  # Prompts are loaded from Langfuse by name and label
  prompts:
    triage_quality:
      name: "evaluation_triage_quality"
      label: "production"
    translation:
      name: "evaluation_translation"
      label: "production"
    kb_relevance:
      name: "evaluation_kb_relevance"
      label: "production"
